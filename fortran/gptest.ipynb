{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520dcd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f68c02a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: [[0.  ]\n",
      " [0.25]\n",
      " [0.5 ]\n",
      " [0.75]\n",
      " [1.  ]] [[0.        ]\n",
      " [0.24740396]\n",
      " [0.47942554]\n",
      " [0.68163876]\n",
      " [0.84147098]]\n",
      "Test data: [[0.33]\n",
      " [0.66]\n",
      " [0.99]]\n",
      "GP prediction at [0.33]: 0.3255984553690249\n",
      "GP gradient at [0.33]: 0.9558227774529487\n",
      "GP gradient of gradient at [0.33]: [-0.58985326] \n",
      "GP prediction at [0.66]: 0.611425137763322\n",
      "GP gradient at [0.66]: 0.797045382858282\n",
      "GP gradient of gradient at [0.66]: [-0.33433219] \n",
      "GP prediction at [0.99]: 0.8368924779938347\n",
      "GP gradient at [0.99]: 0.4682674105907383\n",
      "GP gradient of gradient at [0.99]: [-2.10317375] \n"
     ]
    }
   ],
   "source": [
    "def gp_predict(x_test,x_train,y_train,lengthscale=1.,outputscale=1.,noise=1e-6):\n",
    "    \"\"\"\n",
    "    Predicts the mean and variance of a Gaussian Process at test points given training data.\n",
    "    \n",
    "    Parameters:\n",
    "    - x_test: Test input points (shape: [n_test, d])\n",
    "    - x_train: Training input points (shape: [n_train, d])\n",
    "    - y_train: Training output values (shape: [n_train])\n",
    "    - lengthscale: Lengthscale of the kernel\n",
    "    - outputscale: Output scale of the kernel\n",
    "    - noise: Noise level in the observations\n",
    "    \n",
    "    Returns:\n",
    "    - mean: Predicted mean at test points (shape: [n_test])\n",
    "    - var: Predicted variance at test points (shape: [n_test])\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute squared distances\n",
    "\n",
    "    sqdist = jnp.sum(x_train**2, axis=1, keepdims=True) + jnp.sum(x_train**2, axis=1) - 2 * jnp.dot(x_train, x_train.T)\n",
    "    K = outputscale**2 * jnp.exp(-0.5 * sqdist / lengthscale**2)\n",
    "    # print(\"K shape:\", K.shape)\n",
    "\n",
    "    x_test = jnp.atleast_2d(x_test)\n",
    "\n",
    "    sqdist_12 = jnp.sum(x_test**2, axis=1, keepdims=True) + jnp.sum(x_train**2, axis=1) - 2 * jnp.dot(x_test, x_train.T)\n",
    "    \n",
    "    # Compute covariance matrix\n",
    "    K12 = outputscale**2 * jnp.exp(-0.5 * sqdist_12 / lengthscale**2)\n",
    "    \n",
    "    # print(\"K12 shape:\", K12.shape)\n",
    "\n",
    "    # Add noise to the diagonal\n",
    "    K += noise * jnp.eye(K.shape[0])\n",
    "\n",
    "    # Compute the Cholesky decomposition\n",
    "    L = jnp.linalg.cholesky(K)\n",
    "    \n",
    "    # Solve for alpha\n",
    "    alpha = jax.scipy.linalg.cho_solve((L, True), y_train) #jnp.linalg.solve(L.T, jnp.linalg.solve(L, y_train))\n",
    "    # print(\"Alpha shape:\", alpha.shape)\n",
    "\n",
    "    # Compute the mean at test points\n",
    "    mean = jnp.dot(K12, alpha)\n",
    "    # mean = jnp.einsum('ij,ji',K12.T, alpha)\n",
    "    \n",
    "    # # Compute the variance at test points\n",
    "    # v = jnp.linalg.solve(L, K.T)\n",
    "    # var = outputscale**2 - jnp.sum(v**2, axis=0)\n",
    "    return jnp.reshape(mean,())\n",
    "    # return mean, var\n",
    "\n",
    "n = 5\n",
    "x_train = jnp.array([(i)*0.25 for i in range(n)]).reshape(-1, 1)\n",
    "y_train = jnp.sin(x_train)\n",
    "\n",
    "print(\"Training data:\"  , x_train, y_train)\n",
    "# print(\"Shapes:\", x_train.shape, y_train.shape)\n",
    "\n",
    "x_test = jnp.array([(i)*0.33 for i in range(1,4)]).reshape(-1, 1)\n",
    "\n",
    "print(\"Test data:\"  , x_test)\n",
    "\n",
    "# print(\"Shapes:\", x_test.shape)\n",
    "\n",
    "# mean = gp_predict(x_test, x_train, y_train, lengthscale=0.5, outputscale=1.0, noise=1e-6)\n",
    "\n",
    "# print(\"Predicted mean at test points:\", mean)\n",
    "\n",
    "for x in x_test:\n",
    "    pred = gp_predict(x, x_train, y_train, lengthscale=0.5, outputscale=1.0, noise=1e-6)\n",
    "    print(f\"GP prediction at {x}: {pred}\")\n",
    "    grad = lambda x: jax.grad(gp_predict, argnums=0)(x, x_train, y_train, lengthscale=0.5, outputscale=1.0, noise=1e-6).squeeze()\n",
    "    print(f\"GP gradient at {x}: {grad(x)}\")\n",
    "    # gradient of gradient\n",
    "\n",
    "    print(f\"GP gradient of gradient at {x}: {jax.grad(grad)(x)} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34334b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(x_train, y_train, 'ro', label='Training data')\n",
    "x_test = jnp.linspace(0, 1.5, 100).reshape(-1, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
